{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy &nbsp;&nbsp;&nbsp;&nbsp;: Numerical library used for manipulating vectors like dot product, vector product, sum etc\n",
    "<br>\n",
    "Matplotlib : 2D Ploting library to visulize graphs\n",
    "             Here used to plot image of number with 28x28 dimensional pixel values\n",
    "<br>\n",
    "Pandas &nbsp;&nbsp;&nbsp;: Used for working with tables efficiently\n",
    "             Here used to read .csv file of mnist database\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Applies activation function after output from every layer\n",
    "To find derivative of activators, include \"derivative=True\" in argument\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activator:\n",
    "\n",
    "    def sigmoid(z, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (activator.sigmoid(z=z, derivative=False) * (1 - activator.sigmoid(z=z, derivative=False)))\n",
    "        return (1.0 / (1.0 + np.exp(-z)))\n",
    "\n",
    "    def softmax(z, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (activator.softmax(z=z, derivative=False) * (1 - activator.softmax(z=z, derivative=False)))\n",
    "        return (np.exp(z) / np.sum(np.exp(z)))\n",
    "\n",
    "    def tanh(z, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (activator.tanh(z=z, derivative=False) * (1 - activator.tanh(z=z, derivative=False)))\n",
    "        return (np.tanh(z))\n",
    "\n",
    "    def relu(z, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (float(z>0))\n",
    "        return (np.maximum(z, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "                              O\n",
    "                              O\n",
    "                              O\n",
    "        O                     O\n",
    "        O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     |\n",
    "        O                     O                     |\n",
    "        |                     O                     |\n",
    "        |                     O                     |\n",
    "        |                     O                     |\n",
    "        |                     |                     |\n",
    "        |                     |                     |\n",
    "        |                     |                     |\n",
    "        |                     |                     |\n",
    "        |                     |                     |\n",
    "      Input                Hidden                Output\n",
    "      Layer                Layer                  Layer\n",
    " (inp_size, no_inp)  (hid_size, inp_size)  (out_size, hid_size)\n",
    "    (784, 1000)          (1500, 784)            (10, 1500)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Network : A class which creates a network as given above and initialize weights and biases randomly\n",
    "          Further, netowrk is trained my method gradient descent, grad_descn\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "\n",
    "#   error : List of errors predicted after iterating over all epochs for ploting error vs epochs graph\n",
    "#   error.append(train_error)\n",
    "    error = []\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.num_layers = len(size)\n",
    "#       Initialize biases randomly        \n",
    "        self.biases  = [np.zeros([y, 1]) for y in size[1:]]\n",
    "#       Initialize weights randomly    \n",
    "        self.weights = [np.random.randn(y, x)*0.01 for x, y in zip(size[:-1], size[1:])]\n",
    "\n",
    "#   Iterates forward to generate results predicted by network\n",
    "    def train_feed_forward(self, size, input, activators, mini_batch_size):\n",
    "#       $self.z : List of numpy array of outputs of every neuron\n",
    "        self.z = [np.zeros([y, mini_batch_size]) for y in size[:]]\n",
    "#       i : Used as index for using actiator function from list \"activators\"\n",
    "        i=0\n",
    "#       $self.z[0] : Same as input values\n",
    "        self.z[0] = input\n",
    "#       Total iteration of loop is same as number of layers present in model\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "#           input : Placeholder to calculate prediction by model and self.z\n",
    "#             y   =     m   *   x   +  c\n",
    "#           input = (weight * input) + bias\n",
    "            input = (np.dot(weight, input) + bias)\n",
    "            self.z[i+1] = input\n",
    "#           input = activator(input)\n",
    "            if(activators[i]==\"sigmoid\"):\n",
    "                input = activator.sigmoid(z=input, derivative=False)\n",
    "            elif(activators[i]==\"softmax\"):\n",
    "                input = activator.softmax(z=input, derivative=False)\n",
    "            elif(activators[i]==\"tanh\"):\n",
    "                input = activator.tanh(z=input, derivative=False)\n",
    "            elif(activators[i]==\"relu\"):\n",
    "                input = activator.relu(z=input, derivative=False)\n",
    "            i=i+1\n",
    "#       returns output from last layer\n",
    "        return input\n",
    " \n",
    "    \n",
    "#   Mean squared error\n",
    "#   (predicted - expected)**2\n",
    "    def loss(self, Y, Y_hat, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (Y_hat-Y)\n",
    "        return ((Y_hat - Y) ** 2)\n",
    "    \n",
    "\n",
    "#   Implementation of Gradient Descent Algorithm\n",
    "    def grad_descn(self, size, expected_value, training_data, activators, alpha, mini_batch_size, drop_prob, epochs):\n",
    "#       Result : Numpy Array of One Hot Encoded expected value\n",
    "#                Just like demultiplexers\n",
    "#\n",
    "#                0   1   2   3   4   5   6   7   8   9\n",
    "#\n",
    "#                1   0   0   0   0   0   0   0   0   0\n",
    "#                0   1   0   0   0   0   0   0   0   0\n",
    "#                0   0   1   0   0   0   0   0   0   0\n",
    "#                0   0   0   1   0   0   0   0   0   0\n",
    "#                0   0   0   0   1   0   0   0   0   0\n",
    "#                0   0   0   0   0   1   0   0   0   0\n",
    "#                0   0   0   0   0   0   1   0   0   0\n",
    "#                0   0   0   0   0   0   0   1   0   0\n",
    "#                0   0   0   0   0   0   0   0   1   0\n",
    "#                0   0   0   0   0   0   0   0   0   1\n",
    "#\n",
    "        result = np.zeros([size[-1], len(training_data.T)])\n",
    "        for i in range(len(training_data)):\n",
    "            result[expected_value[0, i], i]=True\n",
    "\n",
    "#       Training the network for $epochs number of times\n",
    "#       Iterates again and again through the same data\n",
    "        for epoch_no in range(epochs):\n",
    "            print(epoch_no)\n",
    "#           $nabla_b : List of Numpy Array of Gradients of every Biases\n",
    "#           $nabla_w : List of Numpy Array of Gradients of every Weights\n",
    "            nabla_b = [np.zeros([y, 1]) for y in size[1:]]\n",
    "            nabla_w = [np.zeros([y, x]) for x, y in zip(size[:-1], size[1:])]\n",
    "#           Splits data into mini batches for training\n",
    "#           Weights and Biases updates after every iterarion over mini batch\n",
    "            for k in range(0, len(training_data), mini_batch_size):\n",
    "                mini_batch = training_data[:, k:k+mini_batch_size]\n",
    "                y = result[:, k:k+mini_batch_size]\n",
    "#               delta_nabla   : Error to be corrected in prediction\n",
    "#                               List of List of Numpy Arrays of delta_nabla_b and delta_nabla_w\n",
    "#                               [delta_nabla_b, delta_nabla_w]\n",
    "#               delta_nabla_b : List of Numpy Arrays of Gradient of biases to be corrected\n",
    "#               delta_nabla_b : List of Numpy Arrays of Gradient of weights to be corrected\n",
    "                delta_nabla = self.find_nabla(size=size, activators=activators, mini_batch=mini_batch, mini_batch_size=mini_batch_size, y=y, alpha=alpha)\n",
    "#               self.biases  = self.biases - (learning_rate * error)\n",
    "#                              for every neurons\n",
    "#               self.weights = self.weights - (learning_rate * error)\n",
    "#                              for every neuron\n",
    "                self.biases  = [b-((alpha/mini_batch_size)*n_b) for b, n_b in zip(self.biases, delta_nabla[0])]\n",
    "                self.weights = [w-((alpha/mini_batch_size)*n_w) for w, n_w in zip(self.weights, delta_nabla[1])]\n",
    "\n",
    "#               y_hat : Result predicted by model for current mini batch to calculate error\n",
    "                y_hat = test_feed_forward(size=size, input=mini_batch, activators=activators)\n",
    "#               train_error : Sum of errors of predictions of current mini batch\n",
    "                train_error = np.sum((1/mini_batch_size)*self.loss(Y=y, Y_hat=y_hat))\n",
    "#               print(train_error)\n",
    "                self.error.append(train_error)\n",
    "\n",
    "#   On every iteration of mini batch, control is parsed to find gradient of current mini batch and back propogation\n",
    "    def find_nabla(self, size, activators, mini_batch, mini_batch_size, y, alpha):\n",
    "#       y_hat : Result predicted by model for back propogation\n",
    "        y_hat = self.train_feed_forward(size=size, input=mini_batch, activators=activators, mini_batch_size=mini_batch_size)\n",
    "#       cost : Mean of loss over current mini batch\n",
    "        cost = (1/mini_batch_size)*self.loss(Y=y, Y_hat=y_hat)\n",
    "#       error : Sum of cost of outputs\n",
    "#       delta_nabla_b : List of Numpy Array of error of gradient of biases  of every neurons \n",
    "#       delta_nabla_w : List of Numpy Array of error of gradient of weights of every neurons \n",
    "        delta_nabla_b = [np.zeros([y, 1]) for y in size[1:]]\n",
    "        delta_nabla_w = [np.zeros([y, x]) for x, y in zip(size[:-1], size[1:])]\n",
    "#       Iteration over every elements of mini batch\n",
    "#       delta : Numpy Array of Error of prediction in last layer of every elements in mini batch\n",
    "        if activators[-1] == \"sigmoid\":\n",
    "            delta = self.loss(Y=y, Y_hat=y_hat, derivative=True) * activator.sigmoid(z=y_hat, derivative=True)\n",
    "        elif activators[-1] == \"softmax\":\n",
    "            delta = self.loss(Y=y, Y_hat=y_hat, derivative=True) * activator.softmax(z=y_hat, derivative=True)\n",
    "        elif activators[-1] == \"tanh\":\n",
    "            delta = self.loss(Y=y, Y_hat=y_hat, derivative=True) * activator.tanh(z=y_hat, derivative=True)\n",
    "        elif activators[-1] == \"relu\":\n",
    "            delta = self.loss(Y=y, Y_hat=y_hat, derivative=True) * activator.relu(z=y_hat, derivative=True)\n",
    "        delta_nabla_b[-1] += np.sum(delta)\n",
    "        delta_nabla_w[-1] += np.dot(delta, self.z[-2].T)\n",
    "#       Back Propogation Algorithm : Iteration over all the layers from back\n",
    "        for layer_no in range(-1, -self.num_layers+2, -1):\n",
    "#           delta : Numpy Array of Error of prediction in current layer of every elements in mini batch\n",
    "            if activators[layer_no] == \"sigmoid\":\n",
    "                delta = np.dot(self.weights[layer_no].T, delta) * activator.sigmoid(z=self.z[layer_no-1], derivative=True)\n",
    "            elif activators[layer_no] == \"softmax\":\n",
    "                delta = np.dot(self.weights[layer_no].T, delta) * activator.softmax(z=self.z[layer_no-1], derivative=True)\n",
    "            elif activators[layer_no] == \"tanh\":    \n",
    "                delta = np.dot(self.weights[layer_no].T, delta) * activator.tanh(z=self.z[layer_no-1], derivative=True)\n",
    "            elif activators[layer_no] == \"relu\":\n",
    "                delta = np.dot(self.weights[layer_no].T, delta) * activator.relu(z=self.z[layer_no-1], derivative=True)\n",
    "            delta_nabla_b[layer_no-1] += np.sum(delta)\n",
    "            delta_nabla_w[layer_no-1] += np.dot(delta, self.z[layer_no-2].T)\n",
    "            drop_out = np.random.rand(delta_nabla_w[layer_no]) < drop_prob\n",
    "            delta_nabla_w = np.multiply(delta_nabla_w, drop_out)\n",
    "            delta_nabla_w /= drop_prob\n",
    "        delta_nabla = [delta_nabla_b, delta_nabla_w]\n",
    "        return delta_nabla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Feed Forward Network for test data\n",
    "def test_feed_forward(size, input, activators):\n",
    "       i=0\n",
    "       for bias, weight in zip(mnist.biases, mnist.weights):\n",
    "           input = (np.dot(weight, input) + bias)\n",
    "           if activators[i]==\"sigmoid\":\n",
    "               input = activator.sigmoid(z=input, derivative=False)\n",
    "           elif activators[i]==\"softmax\":\n",
    "               input = activator.softmax(z=input, derivative=False)\n",
    "           elif activators[i]==\"tanh\":\n",
    "               input = activator.tanh(z=input, derivative=False)\n",
    "           elif activators[i]==\"relu\":\n",
    "               input = activator.relu(z=input, derivative=False)\n",
    "           i=i+1\n",
    "       return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   train_data_frame : Pandas dataframe of mnist dataset in csv file\n",
    "train_data_frame = pd.read_csv('/home/pushpull/mount/intHdd/Project/ml/mnist/mnist_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Conversion of Pandas dataframe into numpy arrays\n",
    "train_dataset = np.array(train_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "It is good practice to first shuffle the data randomly to avoid fitting the model for some particular output\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(1, 60000)\n",
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "#   Shuffling the data using \"random\" library\n",
    "np.random.shuffle(train_dataset)\n",
    "\n",
    "train_lable = np.array([train_dataset[:, 0]])\n",
    "train_data = np.array(train_dataset[:, 1:785]).T\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(train_lable.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f824df3e668>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_dataset[0, 1:785].reshape(28, 28), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   neuron_layer : Dictionary of number of neurons in model and activation functions for every layer\n",
    "neuron_layer = {\"size_layers\": [784, 2100, 10], \"activations\": [\"tanh\", \"sigmoid\"] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Initializing network with random weights and biases\n",
    "mnist = network(neuron_layer[\"size_layers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#   Training network using Gradient Descent Algorithm\n",
    "mnist.grad_descn(size=neuron_layer[\"size_layers\"], expected_value=train_lable, training_data=train_data, activators=neuron_layer[\"activations\"], alpha=0.01, mini_batch_size=2000, drop_prob=0.9, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_frame = pd.read_csv('/home/pushpull/mount/intHdd/Project/ml/mnist/mnist_test.csv', header=None)\n",
    "\n",
    "test_dataset = np.array(test_data_frame)\n",
    "\n",
    "test_lable = np.array([test_dataset[:, 0]]).T\n",
    "test_data = np.array(test_dataset[:, 1:785])\n",
    "\n",
    "result = test_feed_forward(size=neuron_layer[\"size_layers\"], input=test_data.T, activators=neuron_layer[\"activations\"])\n",
    "\n",
    "no_trues = 0\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    max_ans = result[0, i]\n",
    "    max_ind = 0\n",
    "    for j in range(10):\n",
    "        if(result[j, i]>max_ans):\n",
    "            max_ind = j\n",
    "            max_ans = result[j, i]\n",
    "    if(test_lable[i]==max_ind):\n",
    "        no_trues+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_dataset[22, 1:785].reshape(28, 28), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_lable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_lable[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100.0*(no_trues/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[91, 0:785].reshape(28, 28), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mnist.error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
