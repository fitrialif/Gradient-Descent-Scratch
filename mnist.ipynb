{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy &nbsp;&nbsp;&nbsp;&nbsp;: Numerical library used for manipulating vectors like dot product, vector product, sum etc\n",
    "<br>\n",
    "Matplotlib : 2D Ploting library to visulize graphs\n",
    "             Here used to plot image of number with 28x28 dimensional pixel values\n",
    "<br>\n",
    "Pandas &nbsp;&nbsp;&nbsp;: Used for working with tables efficiently\n",
    "             Here used to read .csv file of mnist database\n",
    "<br>\n",
    "Random &nbsp;&nbsp;: Generate pseudo random numbers\n",
    "             Here used for shuffeling data in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Applies activation function after output from every layer\n",
    "To find derivative of activators, include \"derivative=True\" in argument\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activator:\n",
    "\n",
    "    def sigmoid(z, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (activator.sigmoid(z=z, derivative=False) * (1 - activator.sigmoid(z=z, derivative=False)))\n",
    "        return (1.0 / (1.0 + np.exp(-z)))\n",
    "\n",
    "    def softmax(z, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (activator.softmax(z=z, derivative=False) * (1 - activator.softmax(z=z, derivative=False)))\n",
    "        return (np.exp(z) / np.sum(np.exp(z)))\n",
    "\n",
    "    def tanh(z, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (activator.tanh(z=z, derivative=False) * (1 - activator.tanh(z=z, derivative=False)))\n",
    "        return (np.tanh(z))\n",
    "\n",
    "    def relu(z, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (float(z>0))\n",
    "        return (np.maximum(z, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "                              O\n",
    "                              O\n",
    "                              O\n",
    "        O                     O\n",
    "        O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     O\n",
    "        O                     O                     |\n",
    "        O                     O                     |\n",
    "        |                     O                     |\n",
    "        |                     O                     |\n",
    "        |                     O                     |\n",
    "        |                     |                     |\n",
    "        |                     |                     |\n",
    "        |                     |                     |\n",
    "        |                     |                     |\n",
    "        |                     |                     |\n",
    "      Input                Hidden                Output\n",
    "      Layer                Layer                  Layer\n",
    " (inp_size, no_inp)  (hid_size, inp_size)  (out_size, hid_size)\n",
    "    (784, 1000)          (1500, 784)            (10, 1500)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Network : A class which creates a network as given above and initialize weights and biases randomly\n",
    "          Further, netowrk is trained my method gradient descent, grad_descn\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "\n",
    "#   error : List of errors predicted after iterating over all epochs for ploting error vs epochs graph\n",
    "#   error.append(train_error)\n",
    "    error = []\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.num_layers = len(size)\n",
    "#       Initialize biases randomly        \n",
    "        self.biases  = [np.zeros([y, 1]) for y in size[1:]]\n",
    "#       Initialize weights randomly    \n",
    "        self.weights = [np.random.randn(y, x)*0.01 for x, y in zip(size[:-1], size[1:])]\n",
    "\n",
    "#   Iterates forward to generate results predicted by network\n",
    "    def train_feed_forward(self, size, input, activators, mini_batch_size):\n",
    "#       $self.z : List of numpy array of outputs of every neuron\n",
    "        self.z = [np.zeros([y, mini_batch_size]) for y in size[:]]\n",
    "#       i : Used as index for using actiator function from list \"activators\"\n",
    "        i=0\n",
    "#       $self.z[0] : Same as input values\n",
    "        self.z[0] = input\n",
    "#       Total iteration of loop is same as number of layers present in model\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "#           input : Placeholder to calculate prediction by model and self.z\n",
    "#             y   =     m   *   x   +  c\n",
    "#           input = (weight * input) + bias\n",
    "            input = (np.dot(weight, input) + bias)\n",
    "            self.z[i+1] = input\n",
    "#           input = activator(input)\n",
    "            if(activators[i]==\"sigmoid\"):\n",
    "                input = activator.sigmoid(z=input, derivative=False)\n",
    "            elif(activators[i]==\"softmax\"):\n",
    "                input = activator.softmax(z=input, derivative=False)\n",
    "            elif(activators[i]==\"tanh\"):\n",
    "                input = activator.tanh(z=input, derivative=False)\n",
    "            elif(activators[i]==\"relu\"):\n",
    "                input = activator.relu(z=input, derivative=False)\n",
    "            i=i+1\n",
    "#       returns output from last layer\n",
    "        return input\n",
    " \n",
    "    \n",
    "#   Mean squared error\n",
    "#   (predicted - expected)**2\n",
    "    def loss(self, Y, Y_hat, derivative=False):\n",
    "        if derivative==True:\n",
    "            return (Y_hat-Y)\n",
    "        return ((Y_hat - Y) ** 2)\n",
    "    \n",
    "\n",
    "#   Implementation of Gradient Descent Algorithm\n",
    "    def grad_descn(self, size, expected_value, training_data, activators, alpha, mini_batch_size, epochs):\n",
    "#       Result : Numpy Array of One Hot Encoded expected value\n",
    "#                Just like demultiplexers\n",
    "#\n",
    "#                0   1   2   3   4   5   6   7   8   9\n",
    "#\n",
    "#                1   0   0   0   0   0   0   0   0   0\n",
    "#                0   1   0   0   0   0   0   0   0   0\n",
    "#                0   0   1   0   0   0   0   0   0   0\n",
    "#                0   0   0   1   0   0   0   0   0   0\n",
    "#                0   0   0   0   1   0   0   0   0   0\n",
    "#                0   0   0   0   0   1   0   0   0   0\n",
    "#                0   0   0   0   0   0   1   0   0   0\n",
    "#                0   0   0   0   0   0   0   1   0   0\n",
    "#                0   0   0   0   0   0   0   0   1   0\n",
    "#                0   0   0   0   0   0   0   0   0   1\n",
    "#\n",
    "        result = np.zeros([size[-1], len(training_data.T)])\n",
    "        for i in range(len(training_data)):\n",
    "            result[expected_value[0, i], i]=True\n",
    "\n",
    "#       Training the network for $epochs number of times\n",
    "#       Iterates again and again through the same data\n",
    "        for epoch_no in range(epochs):\n",
    "#           $nabla_b : List of Numpy Array of Gradients of every Biases\n",
    "#           $nabla_w : List of Numpy Array of Gradients of every Weights\n",
    "            nabla_b = [np.zeros([y, 1]) for y in size[1:]]\n",
    "            nabla_w = [np.zeros([y, x]) for x, y in zip(size[:-1], size[1:])]\n",
    "#           Splits data into mini batches for training\n",
    "#           Weights and Biases updates after every iterarion over mini batch\n",
    "            for k in range(0, len(training_data), mini_batch_size):\n",
    "                mini_batch = training_data[:, k:k+mini_batch_size]\n",
    "                y = result[:, k:k+mini_batch_size]\n",
    "#               delta_nabla   : Error to be corrected in prediction\n",
    "#                               List of List of Numpy Arrays of delta_nabla_b and delta_nabla_w\n",
    "#                               [delta_nabla_b, delta_nabla_w]\n",
    "#               delta_nabla_b : List of Numpy Arrays of Gradient of biases to be corrected\n",
    "#               delta_nabla_b : List of Numpy Arrays of Gradient of weights to be corrected\n",
    "                delta_nabla = self.find_nabla(size=size, activators=activators, mini_batch=mini_batch, mini_batch_size=mini_batch_size, y=y, alpha=alpha)\n",
    "#               self.biases  = self.biases - (learning_rate * error)\n",
    "#                              for every neurons\n",
    "#               self.weights = self.weights - (learning_rate * error)\n",
    "#                              for every neuron\n",
    "                self.biases  = [b-((alpha/mini_batch_size)*n_b) for b, n_b in zip(self.biases, delta_nabla[0])]\n",
    "                self.weights = [w-((alpha/mini_batch_size)*n_w) for w, n_w in zip(self.weights, delta_nabla[1])]\n",
    "\n",
    "#               y_hat : Result predicted by model for current mini batch to calculate error\n",
    "                y_hat = test_feed_forward(size=size, input=mini_batch, activators=activators)\n",
    "#               train_error : Sum of errors of predictions of current mini batch\n",
    "                train_error = np.sum((1/mini_batch_size)*self.loss(Y=y, Y_hat=y_hat))\n",
    "#               print(train_error)\n",
    "                self.error.append(train_error)\n",
    "\n",
    "#   On every iteration of mini batch, control is parsed to find gradient of current mini batch and back propogation\n",
    "    def find_nabla(self, size, activators, mini_batch, mini_batch_size, y, alpha):\n",
    "#       y_hat : Result predicted by model for back propogation\n",
    "        y_hat = self.train_feed_forward(size=size, input=mini_batch, activators=activators, mini_batch_size=mini_batch_size)\n",
    "#       cost : Mean of loss over current mini batch\n",
    "        cost = (1/mini_batch_size)*self.loss(Y=y, Y_hat=y_hat)\n",
    "#       error : Sum of cost of outputs\n",
    "        error = np.sum(cost)\n",
    "#       delta_nabla_b : List of Numpy Array of error of gradient of biases  of every neurons \n",
    "#       delta_nabla_w : List of Numpy Array of error of gradient of weights of every neurons \n",
    "        delta_nabla_b = [np.zeros([y, 1]) for y in size[1:]]\n",
    "        delta_nabla_w = [np.zeros([y, x]) for x, y in zip(size[:-1], size[1:])]\n",
    "\n",
    "#       Iteration over every elements of mini batch\n",
    "#       delta : Numpy Array of Error of prediction in last layer of every elements in mini batch\n",
    "        if activators[-1] == \"sigmoid\":\n",
    "            delta = self.loss(Y=y, Y_hat=y_hat, derivative=True) * activator.sigmoid(z=y_hat, derivative=True)\n",
    "        elif activators[-1] == \"softmax\":\n",
    "            delta = self.loss(Y=y, Y_hat=y_hat, derivative=True) * activator.softmax(z=y_hat, derivative=True)\n",
    "        elif activators[-1] == \"tanh\":\n",
    "            delta = self.loss(Y=y, Y_hat=y_hat, derivative=True) * activator.tanh(z=y_hat, derivative=True)\n",
    "        elif activators[-1] == \"relu\":\n",
    "            delta = self.loss(Y=y, Y_hat=y_hat, derivative=True) * activator.relu(z=y_hat, derivative=True)\n",
    "        delta_nabla_b[-1] += np.sum(delta)\n",
    "        delta_nabla_w[-1] += np.dot(delta, self.z[-2].T)\n",
    "#       Back Propogation Algorithm : Iteration over all the layers from back\n",
    "        for layer_no in range(-1, -self.num_layers+2, -1):\n",
    "#           delta : Numpy Array of Error of prediction in current layer of every elements in mini batch\n",
    "            if activators[layer_no] == \"sigmoid\":\n",
    "                delta = np.dot(self.weights[layer_no].T, delta) * activator.sigmoid(z=self.z[layer_no], derivative=True)\n",
    "            elif activators[layer_no] == \"softmax\":\n",
    "                delta = np.dot(self.weights[layer_no].T, delta) * activator.softmax(z=self.z[layer_no], derivative=True)\n",
    "            elif activators[layer_no] == \"tanh\":    \n",
    "                delta = np.dot(self.weights[layer_no].T, delta) * activator.tanh(z=self.z[layer_no], derivative=True)\n",
    "            elif activators[layer_no] == \"relu\":\n",
    "                delta = np.dot(self.weights[layer_no].T, delta) * activator.relu(z=self.z[layer_no], derivative=True)\n",
    "            delta_nabla_b[layer_no-1] += np.sum(delta)\n",
    "            delta_nabla_w[layer_no-1] += np.dot(delta, self.z[layer_no+1].T)\n",
    "        delta_nabla = [delta_nabla_b, delta_nabla_w]\n",
    "        return delta_nabla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Feed Forward Network for test data\n",
    "def test_feed_forward(size, input, activators):\n",
    "       i=0\n",
    "       for bias, weight in zip(mnist.biases, mnist.weights):\n",
    "           input = (np.dot(weight, input) + bias)\n",
    "           if activators[i]==\"sigmoid\":\n",
    "               input = activator.sigmoid(z=input, derivative=False)\n",
    "           elif activators[i]==\"softmax\":\n",
    "               input = activator.softmax(z=input, derivative=False)\n",
    "           elif activators[i]==\"tanh\":\n",
    "               input = activator.tanh(z=input, derivative=False)\n",
    "           elif activators[i]==\"relu\":\n",
    "               input = activator.relu(z=input, derivative=False)\n",
    "           i=i+1\n",
    "       return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   train_data_frame : Pandas dataframe of mnist dataset in csv file\n",
    "train_data_frame = pd.read_csv('/home/ML/mnist/mnist_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Conversion of Pandas dataframe into numpy arrays\n",
    "train_dataset = np.array(train_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "It is good practice to first shuffle the data randomly to avoid fitting the model for some particular output\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Shuffling the data using \"random\" library\n",
    "random.shuffle(train_dataset)\n",
    "\n",
    "train_lable = np.array([train_dataset[:, 0]])\n",
    "train_data = np.array(train_dataset[:, 1:785]).T\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(train_lable.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset[0, 1:785].reshape(28, 28), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   neuron_layer : Dictionary of number of neurons in model and activation functions for every layer\n",
    "neuron_layer = {\"size_layers\": [784, 1500, 10], \"activations\": [\"tanh\", \"sigmoid\"] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Initializing network with random weights and biases\n",
    "mnist = network(neuron_layer[\"size_layers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Training network using Gradient Descent Algorithm\n",
    "mnist.grad_descn(size=neuron_layer[\"size_layers\"], expected_value=train_lable, training_data=train_data, activators=neuron_layer[\"activations\"], alpha=0.01, mini_batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_frame = pd.read_csv('/home/ML/mnist/mnist_test.csv', header=None)\n",
    "\n",
    "test_dataset = np.array(test_data_frame)\n",
    "\n",
    "test_lable = np.array([test_dataset[:, 0]]).T\n",
    "test_data = np.array(test_dataset[:, 1:785])\n",
    "\n",
    "result = test_feed_forward(size=neuron_layer[\"size_layers\"], input=test_data.T, activators=neuron_layer[\"activations\"])\n",
    "\n",
    "no_trues = 0\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    max_ans = result[0, i]\n",
    "    max_ind = 0\n",
    "    for j in range(10):\n",
    "        if(result[j, i]>max_ans):\n",
    "            max_ind = j\n",
    "            max_ans = result[j, i]\n",
    "    print(i, test_lable[i], max_ind)\n",
    "    if(test_lable[i]==max_ind):\n",
    "        no_trues+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_lable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_lable[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100.0*(no_trues/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[91, 0:785].reshape(28, 28), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mnist.error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
